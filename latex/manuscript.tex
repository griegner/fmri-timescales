\documentclass[10pt]{article}
\usepackage{fullpage} % 1-in margins
\usepackage{amsmath, amsfonts, amssymb} % equations
\usepackage{graphicx} % figures
\usepackage{hyperref} % hyperlinks
\usepackage[style=nature]{biblatex} % citations + references
\addbibresource{fmri-timescales.bib} % zotero bibliography

\title{Estimating fMRI Timescale Maps}
\author{Gabriel Riegner}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Scientific Motivation} 

Brain activity continuously evolves over a range of temporal scales, from fast oscillations of around $100$ Hz to slow fluctuations of $<0.1$ Hz that synchronize large-scale brain functions \cite{buzsaki_neuronal_2004}. At the high frequency range, electrical signals from brain cells link oscillation to synaptic plasticity and long-term consolidation of information \cite{buzsaki_neuronal_2004}. Further, signal autcorrelation decay rates (i.e. timescales) are relatively rapid in early sensory areas and slower in distant cortical regions \cite{murray_hierarchy_2014, gao_neuronal_2020}. At much lower frequencies, this hierarchical structure repeats itself, appearing temporally scale-invariant. Multimodal evidence suggests that slow temporal structures are an essential part of brain organization, shaped by increasing timescales moving from sensory to associative cortical regions \cite{raut_hierarchical_2020, gao_neuronal_2020, hasson_hierarchy_2008}. These results suggest that hierarchical timescales may emerge from properties intrinsic to brain functional anatomy.\\

Further, the spatial gradients of timescales align with functional aspects of brain organization. For example, primary sensory neurons are tightly coupled to changes in the environment, firing rapidly to the onsets of sensory stimuli, and showing relatively short timescales in the tens of milliseconds \cite{runyan_distinct_2017}. In contrast, neurons in decision making and working memory regions (more generally association regions such as prefrontal cortex), require information maintenance over seconds and correspondingly have much longer timescales \cite{zylberberg_mechanisms_2017}. This mechanism is governed by recurrent synaptic excitation balanced by fast feedback inhibition which generates neural signals that show time dependence over many seconds \cite{wang_decision_2008}. A range of such timescales have been identified by electrical recordings of single neurons from non-human animals sampled over different brain locations; yet because of sampling limitations in these experiments, only sparse sets of neurons (usual from cortical regions) can be recorded from. With sparse data it is difficult to infer on the spatial organization of timescales. Testing this possibility requires brain-wide coverage, which is not feasible with invasive electrophysiology.\\

Resting state functional MRI (rsMRI), which measures spontaneous fluctuations of the blood oxygen level-dependent (BOLD) signal, provides non-invasive full-brain coverage of hemodynamic processes at infraslow frequencies $<0.1$ Hz. The BOLD signal does not directly measure neural activity but rather the hemodynamic changes that are linked to neural activity. Therefore, it reflects a mix of both electrophysiological and metabolic processes. Interest in rsMRI has increase substantially since Biswal and colleagues introduced functional connectivity \cite{biswal_functional_1995}. Although functional connectivity -- typically operationalized as the temporal correlations between pairs of brain regions -- is the standard approach for understanding human brain organization, basic timescale properties of rsMRI remain poorly characterised \cite{shinn_functional_2023}. In the present paper we look to rsMRI and ask how timescales of hemodynamic processes are spatially embedded.\\

\subsection{Statistical Motivation}
This application requires estimating timescale parameters at thousands of locations across the brain. Using simulations of autoregressive processes and rsMRI from the Human Connectome Project, we compare timescale estimation methods and their bias, variance, and computational efficiency. Different methods have been used to estimate timescale parameters from neural signals, both using time and frequency domain models \cite{gao_neuronal_2020, raut_hierarchical_2020}. 

\begin{itemize}
    \item why are existing methods insufficient?
    \item what are the elements of a solution?
\end{itemize}

\section{Methods}

\subsection{Notation}

This section introduces the notation and definitions used in the present paper, focusing on discrete stationary time series.

\subsubsection*{Autocorrelation Function for Stationary Time Series}

For a discrete, stationary time series, $\{X_t, t\in\mathbb{Z}\}$, the autocorrelation function (ACF) at lag $k$ is defined as:

\begin{align*}
    \rho(k) = \frac{\mathbb{E}[(X_{t+k} - \mu_X)(X_t - \mu_X)]}{\sigma_X^2},
\end{align*}

where $\mu_X$ is the mean and $\sigma^2_X$ is the variance of the times series. When the process is stationary, $X_t$ is dependent and identically distributed, both the mean and variance are constant over time $t$. (comment on stationary and ACF decay)\\

\subsubsection*{Sample Autocorrelation Function}

The ACF of a finite sample $\{X_1, X_2, ..., X_n\}$ from a stationary time series, at lag $k$ ($0 \le k < n$), is estimated by:
\begin{align*}
    \hat \rho(k) = \frac{\sum_{t=1}^{n-k}(X_{t+k} - \bar X)(X_t - \bar X)}{\sum_{t=1}^n(X_t - \bar X)^2},
\end{align*}

where $\bar X$ represents the sample mean of the time series.

\subsection{Estimating Timescales}
Different methods have been used to estimate timescale parameters from neural signals, both using time and frequency domain models.\\

\subsubsection*{Time Domain}
\paragraph{(a) Decay rate of the sample ACF, estimated by nonlinear least-squares (NLS)}
\begin{align*}
    \hat \tau = \underset{{\tau}}{\text{min}} \sum_{k=0}^{n-1}[\hat \rho(k) - e^{-\frac{k}{\tau}}]^2,
\end{align*}

where $e^{-\frac{k}{\tau}}$ is an exponential function with decay rate $\tau$. 

\paragraph{(b) Best AR(1) model projection, estimated by ordinary least-squares (OLS)}
\begin{align*}
    \hat \phi &= \underset{\phi}{\text{min}} \ \mathbb{E}[(X_t - \phi X_{t-1})^2]\\
    \hat \tau &= -\frac{1}{\text{log}(\hat \phi)}
\end{align*}

\subsubsection*{Frequency Domain}
\paragraph{(c) PSD Knee Frequency, estimated by nonlinear least-squares (NLS)}
\begin{align*}
    ...
\end{align*}
Procedure that attempts to fit to the aperiodic aspects of the spectrum. scale-free ($\frac{1}{f}$) structure of spectral brain dynamics (more energy in low frequencies).

\subsection{Simulations}  
Realizations of stationary autoregressive time series were simulated to match...\\
filter which is convolved with a one-dimensional white noise signal...

\subsection{Evaluation}

\subsection{Applications}

\section{Results}

\section{Discussion}

\section{Appendix}


\subsection{Delta method}

The standard errors of the timescale parameter is a function of the autocorrelation coefficient $\phi_1$ of an AR(1) process.

The AR(1) process is defined as:  

$X_t = \phi_1 X_{t-1} + \epsilon_t$

where $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$.

The theoretical autocorrelation function (ACF) of the AR(1) process is:

$\rho_\tau = \phi_1^\tau$

where $\rho_\tau$ is the autocorrelation at lag $\tau$.

...

The \textit{timescale} is defined as the number of lags it takes the ACF to reach $\frac{1}{e}$, which is equivalent to finding the value of $\tau$ that satisfies the equation:

$\phi_1^\tau = \frac{1}{e}$

$\tau \cdot log(\phi_1) = log(\frac{1}{e}) = -1$

$\tau = - \frac{1}{log(\phi_1)}$.

...

To apply the delta method, we express the timescale $\tau$ as a function of the AR(1) coefficient $\phi_1$:

$g(\phi_1) = \frac{-1}{log(\phi_1)}$

$g'(\phi_1) = \frac{1}{\phi_1 log(\phi_1)^2}$.

Then estimate the standard error of the estimated timescale parameter:

$\hat{\sigma}_\tau = g'(\hat{\phi_1}) \cdot \hat{\sigma}_{\phi_1}$

where $\hat{\phi_1}$ is the estimate of the AR(1) coefficient and $\hat{\sigma}_{\phi_1}$ is its estimated standard error.

\subsection{Stationarity of AR(2) process}

\href{https://stats.stackexchange.com/questions/118019/a-proof-for-the-stationarity-of-an-ar2}{StackExchange}

An AR(2) process is stable if the following three conditions are met:
1. $\phi_2 < 1 + \phi_1$
2. $\phi_2 < 1 - \phi_1$
3. $\phi_2 > -1$

\subsection{Spectrum of AR(2) process}

\href{https://en.wikipedia.org/wiki/Autoregressive_model#Spectrum}{Wikipedia}

PSD of AR(p) process with noise variance $Var(X_t) = \sigma^2_{X_t}$ is

\begin{align*}
S(f) = \frac{\sigma^2_X}{|1 - \sum_{k=1}^p \phi_k e^{-i2\pi fk}|^2}
\end{align*}

For AR(2)

\begin{align*}
S(f) = \frac{\sigma^2_X}{1 + \phi_1^2 + \phi_2^2 - 2\phi_1(1 - \phi_2)\cos(2\pi f) - 2\phi_2\cos(4\pi f)}
\end{align*}

\subsection{Optimization functions}

NLS:

$\min_{\tau} \int \left[ \rho(t) - e^{-\frac{t}{\tau}} \right]^2 \, dt$  

where $\rho(t)$ is the autocorrelation function (ACF) and $e^{-\frac{t}{\tau}}$ represents an exponential function with a decay rate determined by the parameter $\tau$.

First differentiate the objective function with respect to $\tau$:  

$\frac{d}{d\tau} \left[ \rho(t) - e^{-\frac{t}{\tau}} \right]^2 = - \frac{2 \left[\rho{\left(t \right)} - e^{- \frac{t}{\tau}}\right] e^{- \frac{t}{\tau}}}{\tau^{2}}$  

Next, integrate the derivative expression with respect to $t$:  

$\frac{d}{d\tau} \int \left[ \rho(t) - e^{-\frac{t}{\tau}} \right]^2 \, dt = - \frac{2 \left[\int \left(- e^{- \frac{2 t}{\tau}}\right)\, dt + \int \rho{\left(t \right)} e^{- \frac{t}{\tau}}\, dt\right]}{\tau^{2}}$

?? Can this be solved numerically ??

...

OLS:

$x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \epsilon$  
$\min_{\phi} \mathbb{E} \left[ x_t - \phi x_{t-1} \right]^2$  
$\frac{d}{d\phi} = $

...

Expand the objective function:

$\mathbb{E} \left[ x_t - \phi x_{t-1} \right]^2 = \mathbb{E} \left[ (x_t - \phi x_{t-1})^2 \right]$

Substitute in AR(2) model:

$\mathbb{E} \left[ (x_t - \phi x_{t-1})^2 \right] = \mathbb{E} \left[ (\phi_1 x_{t-1} + \phi_2 x_{t-2} + \epsilon - \phi x_{t-1})^2 \right]$

Take derivative of objective function with respect to $\phi$:

Set derivative equal to zero and solve:

...

% References
\printbibliography


\end{document}