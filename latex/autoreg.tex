\documentclass[latex/main.tex]{subfiles}

\begin{document}
\section{Autoregression}

\subsection{First-Order Autoregression}

A first-order autoregression, denoted AR(1), is

\begin{align}
    X_t = \phi X_{t-1} + e_t \label{eq:ar1-1}
\end{align}

where $\mathbb{E}[e_t] = 0$, $\mathbb{E}[e_t^2] = \sigma^2$.\\
(is $\mathbb{C}[e_t, e_{t-k}]) = 0\text{ for }k\ne0$ necessary?)\\

The moments for a stationary and ergodic AR(1), when $|\phi| < 1$, can be derived from \eqref{eq:ar1-1}.


\textbf{mean}:
\begin{align*}
    \mathbb{E}[X_t] &= \phi \mathbb{E}[X_{t-1}] + \mathbb{E}[e_t] \quad\text{(since $X_{t-1}$ and $e_t$ are independent)}\\
    \mu &= \phi \mu + 0 \quad\text{(by stationarity $\mathbb{E}[X_t] = \mathbb{E}[X_{t-1}] = \mu$)}\\
    &= 0
\end{align*}


\textbf{variance}:
\begin{align*}
    \mathbb{V}[X_t] &= \mathbb{V}[\phi X_{t-1} + e_t]\\
    &= \phi^2 \mathbb{V}[X_{t-1}] + \mathbb{V}[e_t] \quad \text{(since $X_{t-1}$ and $e_t$ are independent)}\\
    &= \phi^2 \mathbb{V}[X_{t-1}] + \sigma^2 \quad \text{(by the definition of $e_t$)}\\
    \gamma_0 &= \phi^2 \gamma_0 + \sigma^2 \quad \text{(by stationarity $\mathbb{V}[X_t] = \mathbb{V}[X_{t-1}] = \gamma_0$)}\\
    &= \frac{\sigma^2}{(1 - \phi^2)}
\end{align*}


\textbf{autocovariance}:
\begin{align*}
    \mathbb{C}[X_t,X_{t-k}] &= \mathbb{E}[(\phi X_{t-1} + e_t) X_{t-k}]\\
    &= \phi \mathbb{E}[X_{t-1}X_{t-k}] + \mathbb{E}[e_t X_{t-k}]\\
    &= \phi \mathbb{E}[X_{t-1} X_{t-k}] + 0 \quad \text{($\mathbb{E}[e_t X_{t-k}] = 0$ for $k>0$)}\\
    \gamma_k &= \phi \gamma_{k-1} \quad \text{(by stationarity, $\mathbb{E}[X_{t-1}X_{t-k}] = \gamma_{k-1}$)}
\end{align*}

By recursion we obtain the Yule-Walker equation for an AR(1)
\begin{align*}
    \gamma_1 &= \phi \gamma_0 \quad \text{(where $\gamma_0 = \mathbb{V}[X_t]$)}\\
    \gamma_2 &= \phi\gamma_1 = \phi(\phi\gamma_0) = \phi^2\gamma_0\\
    \gamma_3 &= \phi\gamma_2 = \phi(\phi^2\gamma_0) = \phi^3\gamma_0\\
    &...\\
    \gamma_k &= \phi^k\gamma_0\\
    \rho_k &= \phi^k \quad \text{(since $\rho_k = \gamma_k / \gamma_0$)}
\end{align*}

Thus, stationary AR(1) processes have autocorrelations which decay geometrically to zero as $k$ increases.

\subsection{Least Squares Estimation of AR Models}

Under mild conditions (stationarity and ergodicity), the coefficients of an AR(1) model can be \textbf{consistently} estimated by least squares, although estimates are \textbf{biased} for small samples. This holds for any stationary and ergodic process, not only autoregressive processes.\\

The least squares estimates of the AR(1) model are
\begin{align}
    \hat \phi &= (\sum_{t=2}^T X_{t-1}^2)^{-1} \sum_{t=2}^T X_{t-1} X_t \label{eq:phi-hat}\\
    \hat e_t &= X_t - X_{t-1} \hat \phi \label{eq:e-hat}\\
    \hat \sigma^2 &= \frac{1}{T-1} \sum_{t=2}^T \hat e_t^2 \label{eq:var-hat}\\
    \text{se}(\hat \phi) &= \sqrt{\hat \sigma^2 (\sum_{t=2}^T X_{t-1}^2)^{-1}} \label{eq:se-hat}
\end{align}

This estimator has the following properties

\textbf{Bias}\\

\begin{align*}
    \hat \phi &= \frac{\sum_{t=2}^T X_{t-1} X_t}{\sum_{t=2}^T X_{t-1}^2}\\
    &= \frac{\sum_{t=2}^T X_{t-1} (\phi X_{t-1} + e_t)}{\sum_{t=2}^T X_{t-1}^2}\\
    &= \frac{\sum_{t=2}^T X_{t-1}(\phi X_{t-1})}{\sum_{t=2}^T X_{t-1}^2}\\
    &= \phi \frac{\sum_{t=2}^T X_{t-1}^2}{\sum_{t=2}^T X_{t-1}^2} + \frac{\sum_{t=2}^T X_{t-1} e_t}{\sum_{t=2}^T X_{t-1}^2}\\
    &= \phi + \sum_{t=2}^T (\frac{X_{t-1}}{\sum_{t=2}^T X_{t-1}^2}) e_t
\end{align*}

Therefore, $\mathbb{E}[X_{t-1}e_t] = 0$ because $e_t$ is independent of $X_{t-1}$. But $\mathbb{E}[\hat \phi] < \phi$ because $e_t$ is not independent of the sum $\sum_{t=2}^T X_{t-1}^2$. There is a negative correlation between $e_t$ and $X_{t-1}/\sum_{t=2}^T X_{t-1}^2$.\\

\textbf{Consistency}\\

By Ergodic Theorem (cite)
\begin{align*}
    \hat\phi = (\sum_{t=2}^T X_{t-1}^2)^{-1} \sum_{t=2}^T X_{t-1} X_t \underset{p}{\to}
    (\mathbb{E}[X_{t-1}^2])^{-1} \mathbb{E}[X_{t-1} X_t] = \phi
\end{align*}

*Show the same for $\hat \sigma^2$\\

Therefore, the least squares estimator is biased ($\mathbb{E}[\hat\phi] \ne \mathbb{E}[\phi]$), but consistent $\hat \phi \underset{p}{\to} \phi$. Note that this does not require that the time series $\{X_t\}$ is an AR(1) process, but holds for any stationary and ergodic process.\\


\textbf{Asymptotic Distribution of Least Squares Estimator}

If $e_t$ is a Martingale Difference Sequence (MDS; cite definition), by the Central Limit Theorem for a MDS (cite)
\begin{align*}
    \sqrt{T}(\hat \phi - \phi) &\underset{d}{\to} \mathcal{N}(0, \sigma^2 \mathbb{E}[X_{t-1}^2])\\
    &= \mathcal{N}(0, 1- \phi^2) \quad (\text{for an AR(1), $\mathbb{E}[X_{t-1}^2] = \sigma^2 / (1-\phi^2)$)}
\end{align*}

as $T \to \infty$.\\

This, the asymptotic variance depends only on $\phi$ and decreases with $\phi^2$, implying that larger $\phi$ can be estimated more precisely. Yet, if the AR(1) model is an approximation or misspecification, then the above central limit theorem does not apply. \\

Instead, if we assume that the errors $e_t$ are serially correlated, and $\{X_t\}$ is strong mixing (cite definition), the Central Limit Theorem for mixing processes (cite) defines an asymptotic distribution under general dependence.

\begin{align*}
    \sqrt{T}(\hat\phi - \phi) \underset{d}{\to} \mathcal{N}(0, V)
\end{align*}

as $T\to\infty$, where $V = Q^{-1} \Omega Q^{-1}$.\\

Let $\gamma_{x,k}$ denote the autocovariance function of $X_t$ at lag $k$, and $\gamma_{e,k}$ denote the autocovariance function of $e_t$ at lag $k$. 

\begin{align*}
    \Omega &= \sum_{k=-\infty}^\infty \mathbb{E}[X_{t-1-k}X_{t-1}e_te_{t-k}]\\
    &= \sum_{k=-\infty}^\infty \gamma_{x,k} \gamma_{e,k} \quad \text{(by stationarity $\mathbb{E}[X_{t-1-k}X_{t-1}] = \gamma_{x,k}$)}\\
    &= \sum_{k=-\infty}^\infty \phi^k \gamma_{x,0} \gamma_{e,k} \quad \text{(by the autocovariance of AR(1))}\\
    &= \sum_{k=-\infty}^\infty \phi^k \frac{\sigma^2}{(1-\phi^2)} \gamma_{e,k} \quad \text{(by the variance of AR(1))}\\\\
    V &= \frac{(1-\phi^2)}{\sigma^2} \Omega \frac{(1-\phi^2)}{\sigma^2}\\
    &= [\frac{(1-\phi^2)}{\sigma^2}] \sum_{k=-\infty}^\infty \phi^k \frac{\sigma^2}{(1-\phi^2)} \gamma_{e,k} [\frac{(1-\phi^2)}{\sigma^2}]\\
    &= [\frac{(1-\phi^2)}{\sigma^2}] \sum_{k=-\infty}^\infty \phi^k \gamma_{e,k}
\end{align*}

The CLT above simplifies to
\begin{align*}
    \sqrt{T}(\hat\phi - \phi) \underset{d}{\to} \mathcal{N}(0, V)
\end{align*}

as $T\to\infty$, where $V = \frac{(1-\phi^2)}{\sigma^2} \sum_{k=-\infty}^\infty \phi^k e_t e_{t-k}$.\\

\end{document}