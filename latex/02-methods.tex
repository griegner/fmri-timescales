\documentclass[main.tex]{subfiles}

\begin{document}
\section{Methods}

This section details the two timescale models: the \nameref{sec:time-domain-linear-model} and \nameref{sec:autocorrelation-domain-nonlinear-model}. We present model assumptions, definitions, estimation, standard errors, and estimator properties. Following is a description of the \nameref{sec:simulations} used to validate the theoretical properties of the estimators, and a \nameref{sec:dataset-description} used for method development and the estimation of fMRI timescale maps.

\subsection{Assumptions}

Let $\{X_t, t\in \mathbb{Z}\}$ be a discrete-time stochastic process that is \textbf{weakly stationary} and \textbf{ergodic}, and $x_t = \{x_1, x_2, ..., x_T\}$ be a finite sample of $X_t$. For simplicity, assume $X_t$ and $x_t$ are mean zero. Stationarity implies a constant (independent of time index $t$) mean and variance, and autocovariances that only depend on time lag $k$:

\begin{align*} 
    \gamma_k = \text{cov}[X_t, X_{t-k}] = \mathbb{E}[X_t X_{t-k}].
\end{align*}

\noindent Thus, the autocovariances of a stationary process are a function of the linear dependence between $X_t$ and its lags. By the Cauchy-Schwarz inequality, $|\gamma_k|\le \gamma_0$ for all $k$, meaning that the dependence of a stationary process on its past values tends to weaken with increasing lag. However, stationarity does not impose further restrictions on this behavior, and in constant or periodic processes $\gamma_k$ may not decay.\\

Ergodicity imposes a a stronger restriction on the dependence between observations than stationarity, yet still allows for a wide set of time series processes. Specifically, it requires mixing such that $\underset{T\to\infty}{\text{lim}} \frac{1}{T} \sum_{k=1}^T |\gamma_k| = 0$, and summability such that $\sum_{k=1}^\infty |\gamma_k| < \infty$ \citep[chapter~14.7]{hansen_econometrics_2022}. Together, these conditions guarantee the autocovariances to decay to zero asymptotically at a rate that is absolutely summable. As the time separation between $X_t$ and its lags $X_{t-k}$ increases, the degree of dependence decreases, eventually reaching independence. These conditions allows us to characterize the `memory' or `persistence' of the stochastic process by analyzing the rate of decay of its autocovariances as a function of time lag.\\

For analysis we use a normalized measure of the autocovariances, the \textbf{autocorrelation function (ACF)}:

\begin{align} \label{eq:ACF}
\rho_k = \text{corr}(X_t, X_{t-k}) = (\gamma_0)^{-1}(\gamma_k).
\end{align}

In the neuroscience literature, the most common timescale definition from \citet{murray_hierarchy_2014} is the exponential decay fit of the ACF:
\begin{align} \label{eq:murray}
    \rho_k = \text{exp}(-\frac{k}{\tau}),
\end{align}

\noindent where the timescale parameter $\tau$ controls the decay rate and determines where the autocorrelations reach $1/e \approx 0.37$, known as e-folding time \citep{murray_hierarchy_2014}. This is analogous to the time constants of many physical systems. Implicit in this definition is that the autocorrelations decay to zero exponentially, imposing a stricter requirement than ergodicity, which alone does not guarantee any specific type of decay (exponential, Gaussian, linear, etc.). In the following sections we will relax this assumption to account for approximating models, for when the ACF decay is not strictly exponential, while still capturing the general timescale over which such processes become decorrelated. \\


\subsection{Definitions}
\subsubsection{Time Domain Linear Model}\label{sec:time-domain-linear-model}
The aim here is to describe the exponential decay in autocorrelation of a stationary and ergodic process $X_t$ by a single timescale parameter $\tau$. The exponential decay model from \citet{murray_hierarchy_2014} does this directly using nonlinear methods \ref{eq:murray}. Alternatively, a \textbf{first order autoregressive model (AR1)} provides a convenient linear approximation of the dominant exponential decay pattern, which is implicit in the relationship between the AR1 structure and exponentially decaying autocorrelations, and it is used across a number of neural timescale papers \citep{kaneoke_variance_2012, meisel_decline_2017, huang_timescales_2018, lurie_cortical_2024, shinn_functional_2023, shafiei_topographic_2020}. The AR1 model:

\begin{align}\label{eq:ar1}
    X_t = \phi X_{t-1} + e_t
\end{align}

\noindent defines a parametric regression model where the relationship between $X_t$ and $X_{t-1}$ is linear with respect to the parameter $\phi$. It implies that the theoretical autocorrelation function (ACF) decays exponentially at a rate determined by $\phi$, such that $\rho_k = \phi^k$. Note that this is the same as equation \eqref{eq:murray} with a change of variable, $\tau = g(\phi) = -{\text{log}(|\phi|)}^{-1}$. The absolute value $|\phi|$ is introduced to handle cases where $\phi$ might be negative as the logarithm is only defined for positive arguments. Thus, for a stationary process with $|\phi|<1$, the exponential decay rate can be extrapolated directly from $\phi$, with a timescale equal to the lag at which the AR1 projected ACF reaches $1/e \approx 0.37$. \\

The AR1 projection parameter $\phi^*$ is the value that minimizes the expected squared error function $S(\phi)$, also referred to as the loss or objective function:

\begin{align}
    S(\phi) &= \mathbb{E}[(X_t - \phi X_{t-1})^2]\\
    \phi^* &= \underset{\phi}{\text{argmin}} \; S(\phi).
\end{align}

\noindent $S(\phi)$ is minimized by taking its derivative with respect to $\phi$, setting it to zero, and solving for $\phi^*$:

\begin{align}
    \frac{d}{d\phi} S(\phi) &= -2 \mathbb{E}[X_{t-1}(X_t - \phi X_{t-1})] = 0
\end{align}

\noindent Differentiating this quadratic function yields a linear equation in $\phi$, and solving this results in a closed-form expression for the optimal $\phi^*$. Therefore, $\phi^*$ is defined by \textbf{linear projection} and the timescale parameter $\tau^*$ be a change of variable:

\begin{align}
    \phi^* &= (\mathbb{E}[X_{t-1}^2])^{-1}(\mathbb{E}[X_t X_{t-1}]) \label{eq:ar1-phi}\\
    \tau^* &= g(\phi^*) = -{\text{log}(|\phi^*|)}^{-1} \label{eq:ar1-tau}
\end{align}
\\

Importantly, we do not assume that $X_t$ strictly follows an AR1 process with \textit{iid} errors ($e_t \overset{\text{iid}}{\sim} (0, \sigma^2)$), and this flexibility allows for projection errors that may exhibit unequal variance and residual autocorrelation. Relaxing the constraints on the errors allows for approximating AR1 models where deviations from true AR1 processes are captured by the error term. And since $X_t$ is stationary with finite variance, the parameters $\phi^*$ and $\tau^*$ defined by projection are unique; in fact, any approximating AR1 model is identifiable if $\mathbb{E}[X_{t-1}^2]$ is non-negative \citep[theorem~14.28]{hansen_econometrics_2022}.\\

\subsubsection{Autocorrelation Domain Nonlinear Model}\label{sec:autocorrelation-domain-nonlinear-model}

The autocorrelation domain nonlinear model, as introduced by \citet{murray_hierarchy_2014}, has gained prominence in the neuroscience literature \citep{rossi-pool_invariant_2021, cirillo_neural_2018, ito_cortical_2020, runyan_distinct_2017, zeraati_flexible_2022, nougaret_intrinsic_2021, wasmuht_intrinsic_2018, muller_core_2020, maisson_choice-relevant_2021, li_hierarchical_2022, shafiei_topographic_2020}. For consistent notation with the \nameref{sec:time-domain-linear-model} above, we write the autocorrelation domain nonlinear model as:

\begin{align}
    \rho_k = \phi^k + e_k, \; \text{for}\; k \in \{0, 1, ..., K\},
\end{align}

\noindent where $\rho_k$ denotes the autocorrelation at lag $k$ and $e_k$ is the error term. The relationship between $\rho_k$ and $k$ is nonlinear in the parameter $\phi$ which determines the exponential decay. Note that this definition is nearly identical to the \nameref{sec:time-domain-linear-model}, in that both models describe exponential AR1 decay in autocorrelation, except that the present model defines decay across multiple ($K$) lags of the ACF. This subtle distinction affects the parameterization of $\phi$.\\

The projection parameter $\phi^*$ is again the value that minimizes the expected squared error function $S(\phi)$:

\begin{align}
    S(\phi) &= \mathbb{E}[(\rho_k - \phi^k)^2]\\
    \phi^* &= \underset{\phi}{\text{argmin}} \; S(\phi)
\end{align}

\noindent $S(\phi)$ is minimized by taking its derivative with respect to $\phi$, setting it to zero, and solving for $\phi$:

\begin{align}
    \frac{d}{d\phi} S(\phi) &= -2\mathbb{E}[(k\phi^{k-1})(\rho_k - \phi^k)] = 0
\end{align}

\noindent However, the derivative is nonlinear in $\phi$, preventing a closed-form solution for the least squares minimization. Therefore, numerical methods are needed to find the optimal $\phi^*$, defined by \textbf{nonlinear projection}, which approximates the value minimizing the expected squared error. The corresponding timescale parameter can be expressed as a change of variable:

\begin{align}
    \phi^* &\approx \underset{\phi}{\text{argmin}} \; S(\phi) \label{eq:nlm-phi}\\
    \tau^* &= g(\phi^*) = -{\text{log}(|\phi^*|)}^{-1} \label{eq:ar1-tau}
\end{align}

\subsection{Estimation}
\subsubsection{Time Domain Linear Model Estimator}
The \textbf{linear least squares estimator} of the \nameref{sec:time-domain-linear-model} has the following closed-form expression:

\begin{align}
    \hat\phi_{\scriptscriptstyle\text{LLS}} &= (\sum_{t=2}^T x_{t-1}^2)^{-1} (\sum_{t=2}^T x_t x_{t-1})\\
    \hat\tau_{\scriptscriptstyle\text{LLS}} &= g(\hat\phi_{\scriptscriptstyle\text{LLS}}) = - {\text{log}(|\hat\phi_{\scriptscriptstyle\text{LLS}}|)}^{-1},
\end{align}

\noindent where $\hat\phi_{\scriptscriptstyle\text{LLS}}$ is the linear least squares estimator of an AR1 model \citep[chapter~14.3]{hansen_econometrics_2022}, and $\hat\tau_{\scriptscriptstyle\text{LLS}}$ is the timescale estimator.\\


\subsubsection{Autocorrelation Domain Nonlinear Model Estimator}

Given that the \nameref{sec:autocorrelation-domain-nonlinear-model} is fit to the autocorrelation function (ACF; equation \eqref{eq:ACF}), we first transform the time series data into the autocorrelation domain. The sample ACF for a stationary and ergodic series $x_t = \{x_1, x_2, ..., x_T\}$, assuming it is centered, is estimated by:

\begin{align}\label{eq:ACF_}
    \hat\rho_k &= (\hat\gamma_0)^{-1}(\hat\gamma_k) = (\sum_{t=1}^T x_t^2)^{-1} (\sum_{t=k+1}^{T}x_t x_{t-k}),
\end{align}

\noindent where $\hat\gamma_k$ is the sample covariance at lag $k$ and $\hat\gamma_0$ is the variance.\\


The \nameref{sec:autocorrelation-domain-nonlinear-model} depends on the theoretical ACF from equation \eqref{eq:ACF}, which by ergodicity diminishes to zero as lag $k$ increases. However, due to sampling variability, non-zero autocorrelations will occur when the true value is zero. To mitigate this, the ACF estimator imposes a bias (or regularization) towards zero by scaling estimates by the variance of $x_t$ regardless of lag.\\


Since the ACF decay is nonlinear, the previously presented linear least squares estimator does not apply. Instead, parameters are estimated by \textbf{nonlinear least squares} which falls in the class of numerical optimization methods. The parameter $\phi^*$ that minimizes $S(\phi)$ in equation \eqref{eq:nlm-phi} is estimated by minimizing the sample analog $\hat S(\phi)$:

\begin{align}
    \hat S(\phi) &= \frac{1}{K} \sum_{k=0}^K (\hat\rho_k - \phi^k)^2\\
    \hat \phi^*_{\scriptscriptstyle\text{NLS}} &= \underset{\phi}{\text{argmin}} \; \hat S(\phi)\\
    \hat \tau^*_{\scriptscriptstyle\text{NLS}} &= g(\hat \phi^*_{\scriptscriptstyle\text{NLS}}) = -{\text{log}(|\hat\phi_{\scriptscriptstyle\text{NLS}}|)}^{-1}.
\end{align}

\noindent In this case $\hat \phi^*_{\scriptscriptstyle\text{NLS}}$ does not have an explicit algebraic solution. Consequently, numerical optimization algorithms, such as the Levenberg-Marquardt \citep{watson_levenberg-marquardt_1978} algorithm used in the present paper, iteratively update the estimate of $\hat \phi^*_{\scriptscriptstyle\text{NLS}}$ until convergence. \\

\subsection{Standard Errors}
\subsubsection{Definition: Standard Errors of the Time Domain Linear Model}

In addition to the timescale parameter $\tau^*$, we provide an expression for its standard error under these general conditions. Assuming that $X_t$ is stationary and ergodic, so too are the projection errors from equation \eqref{eq:ar1}, so we can define a standard error for the AR1 coefficient $\phi^*$ that is robust to model misspecification. Further, since the timescale $\tau^*$ can be expressed as a nonlinear function of $\phi^*$, denoted $g(\phi^*)$ in equation \eqref{eq:ar1-tau} with first partial derivative $\frac{d}{d\phi} g(\phi^*)$, the delta method can approximate its standard error:

\begin{align}
    \text{se}_{\text{NW}}(\phi^*) &= \sqrt{q^{-1}\; \omega \; q^{-1}} \label{eq:se-ar1-phi}\\
    \text{se}_{\text{NW}}(\tau^*) &\approx \text{se}_{\text{NW}}(\phi^*) \cdot \frac{d}{d\phi} g(\phi^*),
\end{align}

\noindent where

\begin{align*}
    q = \mathbb{E}[X_{t-1}^2] \quad\text{and}\quad \omega = \sum_{\ell=-\infty}^{\infty} \mathbb{E}[(X_{t-1} \cdot e_t)(X_{t-1-\ell} \cdot e_{t-\ell})].
\end{align*}

The expression takes a sandwich form, as described by \citet{newey_simple_1987}, for 'heteroskedasticity and autocorrelation consistent (HAC)' standard errors \citep[theorem 14.32]{hansen_econometrics_2022}. This formula explicitly adjusts for misspecification by accounting for the covariance structure of the errors, which ensures that the standard errors are asymptotically correct (see \eqref{eq:ar1-phi-clt}). The covariance terms in $\omega$ capture deviations in the error structure from the standard \textit{iid} case. For the special case of correct specification (i.e., $X_t$ is a true AR1 process), the standard error of the AR1 coefficient $\phi^*$ reduces to $\text{se}_{\text{Naive}}(\phi^*) = \sqrt{\sigma^2 q^{-1}}$, where $\sigma^2$ is the error variance. \\

This framework can approximate a general class of decay processes as exponential with standard errors that are robust to model misspecification. Importantly, it allows for the construction of asymptotic approximations, confidence intervals, and null hypothesis tests. Therefore, inferences can be made even when the AR1 model is incorrect.

\subsubsection{Definition: Standard Errors of the Autocorrelation Domain Nonlinear Model}

Although the model defines a parametric regression function with an exponential decay form, it can accommodate deviations from this decay pattern by incorporating the error term $e_k$, allowing for more general stationary and ergodic processes. Following the description in \citet[chapter~22.8, chapter~23.5]{hansen_econometrics_2022}, if $\phi^*$ uniquely minimizes $S(\phi)$, such that $S(\phi) > S(\phi^*)$ for all $\phi \neq \phi^*$, the standard error of $\phi^*$ can be computed using a sandwich form that reflects both the curvature of the squared loss function and the covariance of the errors. Furthermore, since the timescale $\tau$ is a nonlinear function of $\phi^*$, represented by $g(\phi^*)$ in equation \eqref{eq:nlm-tau}, the delta method can provide an approximation of its standard error:

\begin{align}
    \text{se}_\text{NW}(\phi^*) &= \sqrt{q^{-1}\; \omega \;q^{-1}}\\
    \text{se}_\text{NW}(\tau^*) &\approx \text{se}_\text{NW}(\phi^*) \cdot \frac{d}{d\phi}g(\phi^*).
\end{align}

\noindent The components $q$ and $\omega$ are derived from the regression function $m(k, \phi) = \phi^k$ and its derivative $m_{\phi, k} = \frac{d}{d\phi} m(k, \phi) = k \phi^{k-1}$, defined as:

\begin{align*}
    q = \mathbb{E}[m_{\phi^*, k}^2] = \mathbb{E}[(k \phi^{*k-1})^2] \quad\text{and}\quad
    \omega = \sum_{\ell=-\infty}^{\infty} \mathbb{E}[(m_{\phi^*, k} \cdot e_{k})(m_{\phi^*, k-\ell} \cdot e_{k-\ell})].
\end{align*}

The derivative of the regression function evaluated at $\phi^*$ ($m_{\phi^*, k}$) is a 'linearized regressor', and used to locally approximate the nonlinear model by a linear one. As with the linear case (equation \eqref{eq:se-ar1-phi}), this form ensures that the standard errors are asymptotically valid even with model deviations (see \eqref{eq:nls-phi-clt}). This is a realistic scenario under the mild conditions of stationarity and ergodicity. In the correctly specified case, where $e_k \overset{\text{iid}}{\sim} (0, \sigma^2)$, the standard error of $\phi^*$ simplifies to $\text{se}_{\text{Naive}}(\phi^*) = \sqrt{\sigma^2 q^{-1}}$.\\

\subsubsection{Estimation: Standard Errors of the Time Domain Linear Model}

If the model is misspecified and the errors have positive autocorrelation, while it does not bias the estimates of $\hat\phi_{\scriptscriptstyle\text{LLS}}$ or $\hat\tau_{\scriptscriptstyle\text{LLS}}$, the standard errors will be underestimated (and the t-scores overestimated). To consistently estimate the standard errors for more general processes we apply the Newey-West sandwhich formula, and the delta method \citep{newey_simple_1987}:

\begin{align}
\widehat{\text{se}}_{\text{NW}}(\hat\phi_{\scriptscriptstyle\text{LLS}}) &= \sqrt{\hat q^{-1}\;\hat\omega\; \hat q^{-1}}\\
\widehat{\text{se}}_{\text{NW}}(\hat\tau_{\scriptscriptstyle\text{LLS}}) &\approx \widehat{\text{se}}_{\text{NW}}(\hat\phi_{\scriptscriptstyle\text{LLS}}) \cdot \frac{d}{d\phi} g(\hat\phi_{\scriptscriptstyle\text{LLS}})
\end{align}

\noindent where

\begin{align*}
    \hat q = \frac{1}{T} \sum_{t=2}^T x_{t-1}^2 \quad\text{and}\quad
    \hat \omega = \sum_{\ell=-M}^M (1 - \frac{|\ell|}{M+1}) \quad \frac{1}{T} \sum_{1\le t - \ell \le T} (x_{t-1} \cdot \hat e_t)(x_{t-1-\ell} \cdot \hat e_{t-\ell}).
\end{align*}

This estimator calculates a weighted sum of the regression scores $x_{t-1} \cdot \hat e_t$, where $\hat e_t = x_t - \hat\phi_{\scriptscriptstyle\text{LLS}} \cdot x_{t-1}$. The true $\omega$ is approximated by $\hat \omega$ by taking a finite sum of the regression score autocovariances up to lag $M$, where $M$ is the lag-truncation (or bandwidth). The weights used in the sum decrease linearly with lag $\ell$, following a Bartlett kernel. This kernel not only ensures the standard errors remain non-negative but also and regularizes $\hat \omega$ to change smoothly with $M$ \citep[chapter~14.35]{hansen_econometrics_2022}.\\


The model is correctly specified if $x_t$ is generated by an AR1 process, in which case the naive estimator applies:

\begin{align} 
    \widehat{\text{se}}_\text{Naive}(\hat\phi_{\scriptscriptstyle\text{LLS}}) &= \sqrt{\hat\sigma^2 \hat q^{-1}}\\
    \widehat{\text{se}}_\text{Naive}(\hat\tau_{\scriptscriptstyle\text{LLS}}) &= \widehat{\text{se}}_{\text{Naive}}(\hat\phi_{\scriptscriptstyle\text{LLS}}) \frac{d}{d\phi} g(\hat\phi_{\scriptscriptstyle\text{LLS}})
\end{align}

\noindent where $\hat\sigma^2 = \frac{1}{T} \sum_{t=2}^T \hat e_t^2$ is an estimate of the error variance.

\subsubsection{Estimation: Standard Errors of the Autocorrelation Domain Nonlinear Model}
The stability of $\hat\phi^*_{\scriptscriptstyle\text{NLS}}$ at the point of convergence is assessed using a sandwich estimator of the standard error. This approach involves the linearized regressor -- defined as the derivative of the regression function with respect to $\phi$, evaluated at the estimated parameter $\hat \phi^*_{\scriptscriptstyle\text{NLS}}$ -- used as a local linear approximation of the nonlinear model. This approximation facilitates the calculation of standard errors, quantifying the uncertainty in both the location of the regression function and the precision of the parameter estimate. The standard error of the timescale is again approximated by the delta method.

\begin{align}
\widehat{\text{se}}_{\text{NW}}(\hat\phi^*_{\scriptscriptstyle\text{NLS}}) &= \sqrt{\hat q^{-1}\;\hat\omega\; \hat q^{-1}}\\
\widehat{\text{se}}_{\text{NW}}(\hat\tau^*_{\scriptscriptstyle\text{NLS}}) &\approx \widehat{\text{se}}_{\text{NW}}(\hat\phi^*_{\scriptscriptstyle\text{NLS}}) \cdot \frac{d}{d\phi} g(\hat\phi^*_{\scriptscriptstyle\text{NLS}})
\end{align}

\noindent where

\begin{align*}
    \hat q &= \frac{1}{K} \sum_{k=0}^K (\hat m_{\phi,k})^2 = \frac{1}{K} \sum_{k=0}^K (k \hat\phi_{\scriptscriptstyle\text{NLS}}^{*k-1})^2,\\
    \hat \omega &= \sum_{\ell=-M}^M (1 - \frac{|\ell|}{M+1}) \quad \frac{1}{K} \sum_{1 \le k - \ell \le K} (\hat m_{\phi, k} \cdot \hat e_k) (\hat m_{\phi, k-\ell} \cdot \hat e_{k-\ell}).
\end{align*}

This estimator calculates a weighted sum of the linearized regression scores $\hat m_{\phi, k} \cdot \hat e_k$ where $\hat e_k = \hat\rho_k - (\hat\phi^*_{\scriptscriptstyle\text{NLS}})^k$. The estimate of $\hat\omega$ takes a finite sum of these scores up to lag $M$, weighted by a Bartlett kernel, so that $\hat\omega$ changes smoothly with $M$.\\

As discussed previously, naive standard errors will likely be biased downward in practical applications, rendering invalid confidence intervals or hypothesis tests that rely on them. However, in the case of correct specification, the equation simplifies to:

\begin{align} 
    \widehat{\text{se}}_\text{Naive}(\hat\phi^*_{\scriptscriptstyle\text{NLS}}) &= \sqrt{\hat\sigma^2 \hat q^{-1}}\\
    \widehat{\text{se}}_\text{Naive}(\hat\tau^*_{\scriptscriptstyle\text{NLS}}) &\approx \widehat{\text{se}}_{\text{Naive}}(\hat\phi^*_{\scriptscriptstyle\text{NLS}}) \frac{d}{d\phi} g(\hat\phi^*_{\scriptscriptstyle\text{NLS}})
\end{align}

\noindent where $\hat\sigma^2 = \frac{1}{K} \sum_{k=0}^K \hat e_k^2$ is an estimate of the error variance.\\

\subsection{Estimator Properties}
\subsubsection{Properties of Time Domain Linear Model Estimator}
Following the description in \citet[theorem~14.29]{hansen_econometrics_2022}, the ergodic theorem shows that ergodicity is a sufficient condition for \textbf{consistent estimation}. Since $X_t$ is stationary and ergodic, so too are $X_t X_{t-1}$ and $X_{t-1}^2$, and as $T \to \infty$:

\begin{align*}
    \frac{1}{T} \sum_{t=2}^T x_t x_{t-1} &\underset{p}{\to} \mathbb{E}[X_t X_{t-1}]\\
    \frac{1}{T} \sum_{t=2}^T x_{t-1}^2 &\underset{p}{\to} \mathbb{E}[X_{t-1}^2].
\end{align*}

\noindent Applying the continuous mapping theorem yields:
\begin{align*}
    \hat\phi_{\scriptscriptstyle\text{LLS}} = (\frac{1}{T} \sum_{t=2}^T x_{t-1}^2)^{-1} ( \frac{1}{T} \sum_{t=2}^T x_t x_{t-1}) &\underset{p}{\to} (\mathbb{E}[X_{t-1}^2])^{-1}(\mathbb{E}[X_t X_{t-1}]) = \phi.
\end{align*}

\noindent This shows that the coefficients of the \nameref{sec:time-domain-linear-model} can be consistently estimated by least squares, for any stationary and ergodic process with coefficients defined by projection (equation \eqref{eq:ar1-phi}). Similarly:
\begin{align*}
    \hat \omega \underset{p}{\to} \omega
\end{align*}

The asymptotic distribution under model misspecification states we can approximate the \textbf{limiting variance} of $\phi$ using a Central Limit Theorem for correlated observations as $T\to\infty$ \citet[theorem~14.33]{hansen_econometrics_2022}:

\begin{align*} \label{eq:ar1-phi-clt}
\frac{\hat\phi_{\scriptscriptstyle\text{LLS}} - \phi}{\widehat{\text{se}}_{NW}(\hat\phi_{\scriptscriptstyle\text{LLS}})} \underset{d}{\to} \mathcal{N}(0, 1)
\end{align*}

\noindent And by the delta method we obtain the limiting variance for the timescale $\tau$:
\begin{align*}
    \frac{\hat{\tau}_{\scriptscriptstyle\text{LLS}} - \tau}{\widehat{\text{se}}_{NW}(\hat{\phi}_{\scriptscriptstyle\text{LLS}}) \cdot \frac{d}{d\phi} g(\hat \phi_{\scriptscriptstyle\text{LLS}})} \underset{d}{\to} \mathcal{N}(0,1).
\end{align*}

Therefore, when the time domain linear model is incorrectly specified, the asymptotic distribution is still Gaussian with a limiting variance that can consistently be estimated and the resulting t-ratios are asymptotically Gaussian. This allows us to construct hypothesis tests and confidence intervals over timescale maps of the brain.


\subsubsection{Properties of Autocorrelation Domain Nonlinear Model Estimator}
(section under construction)\\

\textbf{Consistency}: if the minimizer $\phi^*$ is unique, $S(\phi) > S(\phi^*)$ for all $\phi \ne \phi^*$, then as $K\to\infty$
\begin{align}
\hat S(\phi) &\underset{p}{\to} S(\phi)\\
\hat \phi^*_{\scriptscriptstyle\text{NLS}} &\underset{p}{\to} \phi^*
\end{align}

And under (??):\\
\begin{align}
\hat \omega \underset{p}{\to} \omega
\end{align}

\textbf{Limiting Variance}: as $K\to\infty$ we can approximate the asymptotic variance of $\phi$ and $\tau$ using a CLT for correlated observations \citep[theorem~23.2]{hansen_econometrics_2022}.

\begin{align}
\frac{\hat\phi^*_{\scriptscriptstyle\scriptscriptstyle\text{NLS}} - \phi^*}{\widehat{\text{se}}_{NW}(\hat\phi^*_{\scriptscriptstyle\text{NLS}})} \underset{d}{\to} \mathcal{N}(0, 1) \label{eq:nls-phi-clt}\\
\frac{\hat{\tau^*} - \tau^*}{\widehat{\text{se}}_{NW}(\hat{\phi^*}_{\scriptscriptstyle\text{NLS}}) \cdot \frac{d}{d\phi}g(\phi^*_{\scriptscriptstyle\text{NLS}})} \underset{d}{\to} \mathcal{N}(0,1)
\end{align}

\subsection{Timescale Maps}
...

\subsection{Simulations}\label{sec:simulations}
 
We simulate samples of stationary and ergodic time series $x_t = \{x_1, x_2, ..., x_T\}$ with three different autocorrelation structures: AR1, AR2, and rfMRI-derived autocorrelations (from subject $\#100610$ of the methods development dataset; see \nameref{sec:dataset-description}). For each of these three types of time dependence, the strength of the autocorrelations were controlled at fixed parameter values $\tau \in \{0.43, 0.77, 1.25, 2.13, 4.48\}$ (and equivalent AR1 projections $\phi \in \{0.1, 0.275, 0.45, 0.625, 0.8\}$) for a total of 15 settings. For each setting, the performance of the \textbf{nonlinear least squares (NLS)} \eqref{eq:ar1-tau} and \textbf{linear least squares (LLS)} \eqref{eq:nlm-tau} estimators were evaluated using $N = 10000$ independent replications of length $T=4800$ timepoints. The empirical bias of parameter and standard error estimates were compared to true values to assess the performance of the estimators, in the sense of sampling bias and variance. \\

Autoregressive correlation structures were generated by applying a recursive infinite impulse response (IIR) filter to Gaussian white noise, $e_t \overset{\text{iid}}{\sim} \mathcal{N}(0, 1)$. Specifically, AR1 and AR2 processes were generated according to the following autoregressive equations:
\begin{align*}
    \text{AR1: }x_t &= \phi_1 x_{t-1} + e_t\\ 
    \phi_1 &\in \{0.1, 0.28, 0.45, 0.62, 0.8\}\\
    \text{AR2: }x_t &= \phi_1 x_{t-1} + \phi_2 x_{t-2} + e_t\\
    [\phi_1, \phi_2] &\in \{[0.09, 0.09], [0.23, 0.18], [0.35, 0.23], [0.47, 0.24], [0.65, 0.19]\}
\end{align*}

\noindent that define the structure of the IIR filter, with parameters as the filter coefficients. \\

The fixed parameter values were chosen based on the range observed by fitting autoregressive models to the methods development dataset (see \nameref{sec:dataset-description}), ensuring stationarity conditions were met. Since empirical data yielded only positive parameter estimates, we fixed the range of simulation parameters to be positive. For AR1 processes, this generates time series with ACFs that decay exponentially, $\rho_k = \phi_1^k$. An AR2 process, though still a linear time process, can have much more complicated stochastic dynamics. Depending on the underlying parameters, this can include signals that are periodic with ACFs that decay as damped cosines or mixes of decaying exponentials. However, because oscillations were absent in the dataset used in the present study -- a characteristic commonly observed in rfMRI signals more broadly \citep{he_scale-free_2011} -- the simulations focused on a subset of stationary, aperiodic AR2 processes.\\

The third setting did not follow an autoregressive process. Instead, it directly inherited autocorrelation structures from five regions $\# \{7, 12, 126, 137, 143\}$ from subject $\#100610$ of the methods development dataset. These regions were selected because they have equivalent AR1 projections to the previous two simulation settings. We estimated the ACF \eqref{eq:ACF_} for each region and represented these estimates as Toeplitz matrices, where the $k^{\text{th}}$ off-diagonal represents the sample ACF at lag $k$. Due to the assumption of stationarity, these matrices take a Toeplitz structure with constant and symmetric diagonals ($T_{i-k, j-k} = T_{i+k, j+k}$). To simulate realizations $x_t = \{x_1, x_2, ..., x_T\}$ with the same ACF as the observed data, we generated Gaussian white noise ($e_t \overset{\text{iid}}{\sim} \mathcal{N}(0, 1)$) multiplied by the Cholesky factor $L$ of the Toeplitz matrix $T$:

\begin{align*}
    T &= LL^T\\
    x_t &= L e_t.
\end{align*}

\subsection{Dataset Description}\label{sec:dataset-description}

Resting fMRI (rfMRI) scans were provided by the Human Connectome Project (HCP), WU-Minn Consortium (led by principal investigators David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers supporting the NIH Blueprint for Neuroscience Research, and by the McDonnell Center for Systems Neuroscience at Washington University \citep{van_essen_wu-minn_2013}. The HCP young adult (ages 22-35) dataset is publicly available under a \href{https://www.humanconnectome.org/storage/app/media/data_use_terms/DataUseTerms-HCP-Open-Access-26Apr2013.pdf}{data usage agreement}. The present paper used two subsets of the larger dataset: one for method development (and defining realistic simulation parameters (see \nameref{sec:simulations}), and the other for estimating high-resolution timescale maps of the cortex and subcortex.\\

For \textbf{method development}, we used the first ten subjects (\#100004 - \#101410) with rfMRI scans acquired using a 3-tesla gradient-echo EPI sequence (TR=720ms, TE=33.1ms, flip angle=52\textdegree, FOV=208x180mm, slice thickness=2.0mm) \citep{van_essen_wu-minn_2013}. Subjects were awake with their eyes open focused on a fixation cross projected on a dark background for four runs (15 minutes each, T=4800 total timepoints). Minimal preprocessing was applied to each run, detailed in \citep{glasser_minimal_2013}. This included anatomical surface reconstruction \citep{robinson_msm_2014} and functional data registered to grayordinates (a spatial map including surface vertices and subcortical gray matter voxels). Additionally, functional time series were preprocessed by applying high-pass filtering, regression of head motion parameters, and removal of temporal artifacts detected by the FIX algorithm \citep{salimi-khorshidi_automatic_2014}. This ensured the removal of nonstationary signal drift and noise (e.g., movement, physiological confounds). To reduce dimensionality, we took weighted spatial averages within 300 brain regions defined by a group-level independent component analysis (ICA) atlas \citep{smith_resting-state_2013}. This yielded a dataset with the dimensions \{10 subjects, 4800 timepoints, 300 regions\}.\\

For \textbf{estimating timescale maps}, we used the subset of 180 HCP subjects with rfMRI scans acquired with a 7-tesla gradient-echo EPI sequence (TR=1000ms, TE=22.2ms, flip angle=45\textdegree, FOV=208 x 208mm, slice thickness=1.6 mm) \citep{van_essen_wu-minn_2013, moeller_multiband_2010}. This sequence offers the highest available spatial resolution for human fMRI, and allows for mapping at the order of 1.6mm$^2$. Four runs of 16 minutes (3600 total timepoints) were collected using the same eyes-open fixation protocol. Preprocessing mirrored the method development dataset. However, functional data were analysed on the grayordinate map, yielding a dataset with the dimensions \{180 subjects, 3600 timepoints, 91282 grayordinates\}. We independently fit the \nameref{sec:time-domain-linear-model} and \nameref{sec:autocorrelation-domain-nonlinear-model} to each grayordinate, a mass-univariate analysis approach that resulted in subject-level maps of timescale estimates and their standard errors.\\

For \textbf{group-level maps}, the timescale estimates and standard errors were combined to account for both within-individual variability and between-individual variability. While remaining within the mass-univariate framework, for simplicity, we express the timescale estimate for the $N=180$ individual subjects at a single grayordinate as:

\begin{align}
    \hat\tau_n \; &\text{for} \; n\in\{1, 2, ..., N\}\\
    \hat\tau_N &= \frac{1}{N} \sum_{n=1}^N \hat\tau_n.
\end{align}

\noindent By the law of total variance, the group-level standard error for the timescale is estimated by:

\begin{align}
    \widehat{\text{se}}(\hat\tau_N) = \sqrt{\frac{1}{N} \sum_{n=1}^N \widehat{\text{se}}(\hat\tau_n)^2 + \frac{1}{N} \sum_{n=1}^N (\hat\tau_n - \hat\tau_N)^2}.
\end{align}

\noindent Here, the first term under the square root is the within-individual variance and the second term is the between-individual variance.\\

To visualize timescales on the brain, statistical parametric maps were constructed using \textbf{t-statistics} at each grayordinate. These t-statistics specifically tested whether timescales exceeded one second  ($H_0: \tau \le 1$), computed as the ratio:

\begin{align}
    t_N = \frac{\hat\tau_N-1}{\widehat{\text{se}}(\hat\tau_N)}.
\end{align}

\noindent Additionally, maps of \textbf{relative standard errors} were used to visualize the spatial distribution in the precision and reliability of timescale estimates across the brain, using the ratio:

\begin{align}
    \text{rse}(\hat\tau_N) = \frac{\widehat{\text{se}}(\hat\tau_N)}{\hat\tau_N}
\end{align}

\end{document}